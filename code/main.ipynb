{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f512263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff2838",
   "metadata": {},
   "source": [
    "#### Provide storage account parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c846d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_conn_string = \"\"\n",
    "src_container = \"\"\n",
    "dst_container = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647b1f1",
   "metadata": {},
   "source": [
    "# Import functions from other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"Data-utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf232a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"FR-Utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"file-utils.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"AzureBlobStorageLib.ipynb\" storage_conn_string src_container dst_container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fe7ec",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "#### Steps include\n",
    "\n",
    "1. Downlaoding data from Blob Storage\n",
    "2. Converting all format files to PDF files\n",
    "3. Splitting multipage PDF to single page PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory and download files\n",
    "\n",
    "#temp_dir = tempfile.TemporaryDirectory()\n",
    "#data_dir = temp_dir.name\n",
    "\n",
    "data_dir = \"../data/\"\n",
    "if os.path.exists(data_dir) :\n",
    "    shutil.rmtree(data_dir)\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a666dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = os.path.join(data_dir,\"rawFiles\")\n",
    "Path(raw_files).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download2local(raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84907a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all file types to pdf and then split to 1-p docs\n",
    "raw_pdf = os.path.join(data_dir,\"allPdf\")\n",
    "Path(raw_pdf).mkdir(parents=True, exist_ok=True)\n",
    "convert2pdf(src = raw_files, pdfdst = raw_pdf)\n",
    "print(\"Input files are stored at:\", raw_pdf)\n",
    "\n",
    "pdf_1p = os.path.join(data_dir,\"1p-pdf\")\n",
    "Path(pdf_1p).mkdir(parents=True, exist_ok=True)\n",
    "pdf_split(src = raw_pdf, dst = pdf_1p)\n",
    "print(\"Processed files are stored at:\", pdf_1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7219171",
   "metadata": {},
   "source": [
    "#### Get initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get initial file count\n",
    "fls = glob.glob(os.path.join(pdf_1p,\"*.pdf\"))\n",
    "initial_file_cnt = len(fls)\n",
    "print(initial_file_cnt)\n",
    "\n",
    "#results directory\n",
    "results_dir = os.path.join(data_dir,\"Results\")\n",
    "Path(results_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce047f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate SAS signature for storage container\n",
    "sas_url = fr_get_sas_url(dst_container)\n",
    "#sas_url = \"\"\n",
    "sas_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6bd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call below function, if you want to clean up the destination blob container\n",
    "#deleteContainerData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbc126",
   "metadata": {},
   "source": [
    "# FR Template identification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3027cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 1\n",
    "\n",
    "while(iteration):\n",
    "    \n",
    "    ########################################################\n",
    "    # create directory for current iteration\n",
    "    ########################################################\n",
    "    iter_fld = \"I\"+str(iteration)\n",
    "    iter_dir = os.path.join(results_dir,iter_fld)\n",
    "    Path(iter_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ########################################################\n",
    "    # sample files for training\n",
    "    ########################################################\n",
    "    train_fld = \"trainset\"\n",
    "    train_dir = os.path.join(iter_dir,train_fld)\n",
    "    Path(train_dir).mkdir(parents=True, exist_ok=True)\n",
    "    sample_training_data(src_fld = pdf_1p, dst_fld = train_dir)\n",
    "\n",
    "    ########################################################\n",
    "    # upload the files to blob\n",
    "    ########################################################\n",
    "    blob_path = os.path.join(iter_fld,train_fld)\n",
    "    upload2blob(local_path = train_dir, container_path = blob_path)\n",
    "    \n",
    "    ########################################################\n",
    "    #train FR unsupervised model\n",
    "    ########################################################\n",
    "    model_file = iter_fld+\"-model-details.json\"\n",
    "    train_fr_model(sas_url = sas_url, folder_path = blob_path.replace(\"\\\\\", \"/\"), model_file = model_file)\n",
    "\n",
    "    ########################################################\n",
    "    #if model is created infer using the model\n",
    "    ########################################################\n",
    "    iter_model_file = os.path.join(iter_dir, model_file)\n",
    "    if os.path.exists(model_file):\n",
    "        shutil.copyfile(model_file, iter_model_file)\n",
    "\n",
    "    infer_fld = \"fr-json\"\n",
    "    infer_dir = os.path.join(iter_dir,infer_fld)\n",
    "    Path(infer_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #Start FR inferencing\n",
    "    fr_model_inference(src_dir = pdf_1p, json_dir = infer_dir, model_file = iter_model_file, thread_cnt = 10)\n",
    "\n",
    "    ########################################################\n",
    "    # Segregate files to clusters\n",
    "    ########################################################\n",
    "    clust_dir = os.path.join(results_dir,\"clusters\")\n",
    "    Path(clust_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cluster_file = os.path.join(iter_dir, iter_fld+\"-clusters.csv\")\n",
    "\n",
    "    files_clustered = segregate_data(src_dir = pdf_1p, result_dir= infer_dir, cluster_dir = clust_dir, \n",
    "                                     prefix = iter_fld, cluster_file = cluster_file)\n",
    "\n",
    "    print(\"Identified clusters for:\", files_clustered, \"files\")\n",
    "\n",
    "    ########################################################\n",
    "    # Upload iteration results to blob storage\n",
    "    ########################################################\n",
    "    \n",
    "    upload2blob(local_path = iter_dir, container_path = iter_fld)  #train data, model details and clusters\n",
    "    upload2blob(local_path = clust_dir, container_path = \"clusters\")  #Files segregated into clusters\n",
    "    \n",
    "    ########################################################\n",
    "    # decide on next iteration\n",
    "    ########################################################\n",
    "    moved_percent = files_clustered * 100 / initial_file_cnt\n",
    "\n",
    "    if (moved_percent < 5) | (initial_file_cnt < 500):\n",
    "        iteration = 0\n",
    "    else:\n",
    "        iteration = iteration + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7eacb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6506c5172df6811f7bf01b57d8469d1357a539c9dc33b21363eaf6ce598c5969"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
