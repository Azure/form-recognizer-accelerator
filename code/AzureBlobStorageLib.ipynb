{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-storage-blob\n",
    "# or !pip install --upgrade --force-reinstall azure.storage.blob\n",
    "# remember to restart kernel / pip list to confirm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish blob connection\n",
    "\n",
    "### The following code establishes connection with Azure Blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Blob Storage v12.9.0\n"
     ]
    }
   ],
   "source": [
    "# Check the version of the client library\n",
    "import os, uuid\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Azure Blob Storage v\" + __version__) # should be at least Azure Blob Storage v12.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please update these variables - \n",
    "###### Only If you are running this notebook separately, otherwise these parameters are picked up from main file\n",
    "\n",
    "### storage_conn_string \"Storage account connection string\"\n",
    "### src_container \"Container where data is stored\"\n",
    "### dst_container \"Container where results should be uploaded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# # set up connection\n",
    "# storage_conn_string = \"\"\n",
    "# src_container = \"\"\n",
    "# dst_container = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or on linux, do: export AZURE_STORAGE_CONNECTION_STRING=\"<yourconnectionstring>\" and then do storage_conn_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "\n",
    "# Create the BlobServiceClient object which will be used to create a container client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(storage_conn_string)\n",
    "\n",
    "# create a client for a specific container\n",
    "srcblob_container_client = blob_service_client.get_container_client(src_container)\n",
    "\n",
    "# create a client for a specific container\n",
    "dstblob_container_client = blob_service_client.get_container_client(dst_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from Blob\n",
    "\n",
    "### The following function downloads data from blob storage to local folder\n",
    "#### parameters\n",
    "\n",
    "local_path         \"Local path where data should be stored\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def download2local(local_path, container_client = srcblob_container_client, container_name = src_container):\n",
    "\n",
    "    # Create a local directory to hold blob data\n",
    "    Path(local_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # download all the files to the local_path folder\n",
    "    blob_list = container_client.list_blobs() # need to run this again, no way to reset the iterator\n",
    "    for blob in blob_list:\n",
    "\n",
    "        # note that this is called on the service_client and not the container_client\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob)\n",
    "\n",
    "        download_file_path = os.path.join(local_path, blob.name)\n",
    "        print(\"\\tDownloading \" + blob.name + \" to \" + download_file_path)\n",
    "\n",
    "        basedir = os.path.split(download_file_path)[0]\n",
    "        Path(basedir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(download_file_path, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "\n",
    "    print(\"Files Downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_path = \"../data/samples\"\n",
    "#download2local(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to Blob container\n",
    "\n",
    "### The following function uploads data from local drive to azure blob storage container\n",
    "#### parameters\n",
    "\n",
    "local_path         \"Local path where data should be stored\" \n",
    "container_path     \"Path on container where data should be stored\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os.path import relpath\n",
    "import glob\n",
    "\n",
    "def upload2blob(local_path, container_path = \"\", container_client = dstblob_container_client, container_name = dst_container):\n",
    "\n",
    "    # list the local files\n",
    "    files_to_upload = glob.glob(os.path.join(local_path,\"**/*\"), recursive=True)\n",
    "\n",
    "    for file_to_upload in files_to_upload:\n",
    "        print(\"\\nUploading to Azure Storage as blob: \\t\" + file_to_upload)\n",
    "\n",
    "        # Upload the file\n",
    "        #path_to_file = os.path.join(local_path, file_to_upload)\n",
    "        path_to_file = file_to_upload\n",
    "        file_path_on_azure = relpath(file_to_upload, local_path)\n",
    "        file_path_on_azure = os.path.join(container_path,file_path_on_azure)\n",
    "\n",
    "        #print(\"local path:\", path_to_file, \"Azure path:\", file_path_on_azure)\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=file_path_on_azure)\n",
    "\n",
    "        path = Path(path_to_file)\n",
    "        if path.is_file():\n",
    "            with open(path_to_file, \"rb\") as data:\n",
    "                blob_client.upload_blob(data, overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = \"../data/samples\"\n",
    "# upload2blob(local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete data in Blob container\n",
    "\n",
    "### The following function deletes all data in azure blob storage container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os.path import relpath\n",
    "import glob\n",
    "\n",
    "def deleteContainerData(container_client = dstblob_container_client, container_name = dst_container):\n",
    "\n",
    "    blob_list = container_client.list_blobs() # need to run this again, no way to reset the iterator\n",
    "    for blob in blob_list:\n",
    "        dstblob_container_client.delete_blob(blob=blob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SAS signature for blob container\n",
    "\n",
    "### This container generates SAS URL for a Blob storage container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import AccessPolicy, ContainerSasPermissions\n",
    "from azure.storage.blob import generate_container_sas, generate_account_sas\n",
    "\n",
    "def fr_get_sas_url(dst_container = dst_container):\n",
    "    #permission = ContainerSasPermissions(read=True, write=True, delete=True, list=True)\n",
    "    permission = ContainerSasPermissions.from_string(storage_conn_string)\n",
    "    expiry=datetime.utcnow() + timedelta(hours=24)\n",
    "    start=datetime.utcnow() - timedelta(minutes=1)\n",
    "\n",
    "    access_policy = AccessPolicy(permission, expiry, start)\n",
    "\n",
    "    sas_token = generate_container_sas(\n",
    "        account_name = blob_service_client.account_name,\n",
    "        container_name = dst_container,\n",
    "        account_key = blob_service_client.credential.account_key,\n",
    "        permission = permission,\n",
    "        expiry = expiry,\n",
    "        start = start\n",
    "    )\n",
    "    \n",
    "    #return(sas_token)\n",
    "    \n",
    "    url = 'https://'+blob_service_client.account_name+'.blob.core.windows.net/'+dst_container+'?'+sas_token\n",
    "    return(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sas_token = fr_get_sas_url()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sas_token"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6506c5172df6811f7bf01b57d8469d1357a539c9dc33b21363eaf6ce598c5969"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
