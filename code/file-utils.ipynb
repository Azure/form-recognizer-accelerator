{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59451e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import random\n",
    "import pandas as pd\n",
    "import json\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e337e",
   "metadata": {},
   "source": [
    "# Data sampling\n",
    "\n",
    "#### This function samples data required for Form Recognizer Training.\n",
    "It sampels such that the file count is < file_limit and Total sample size is < size_limit (in MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_training_data(src_fld, dst_fld, file_limit=500, size_limit=50):\n",
    "\n",
    "    fls = glob.glob(os.path.join(src_fld,\"*.pdf\"))\n",
    "\n",
    "    #sample and copy files\n",
    "    if len(fls) > file_limit:\n",
    "        flist = random.sample(fls,file_limit)\n",
    "        print(\"sampling files\")\n",
    "    else:\n",
    "        flist = fls\n",
    "        print(\"file count <\",file_limit, \"considering all files\")\n",
    "\n",
    "    for fl in flist:\n",
    "        flname = os.path.basename(fl)\n",
    "        copyfile(os.path.join(src_fld, flname), os.path.join(dst_fld, flname))\n",
    "\n",
    "    #delete files if they exceed size limit\n",
    "    fls = os.listdir(dst_fld)\n",
    "    file_n_size = [(f,os.stat(os.path.join(dst_fld, f)).st_size) for f in fls]\n",
    "    #print(file_n_size)\n",
    "    \n",
    "    file_df = pd.DataFrame(file_n_size,columns=['file','size'])\n",
    "    file_df = file_df.sort_values(by=['size'])\n",
    "    file_df['total_size'] = file_df['size'].cumsum()\n",
    "    file_df['to_delete'] = file_df['total_size'] < size_limit*10^6\n",
    "    \n",
    "    del_df = file_df[file_df['to_delete'] == True]\n",
    "    if del_df.shape[0] > 0:\n",
    "        for idx, row in del_df.iterrows():\n",
    "            flname = row['file']\n",
    "            os.remove(os.path.join(dst_fld, flname))\n",
    "    \n",
    "    print(file_df.tail(5))\n",
    "    #return(file_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# sample_training_data(src_fld=\"../data/samples/trainset1\", dst_fld=\"../data/train_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f00bf5",
   "metadata": {},
   "source": [
    "# Segregate data function\n",
    "\n",
    "#### This function segregates data into templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read cluster id and segregate the data\n",
    "\n",
    "def segregate_data(src_dir, result_dir, cluster_dir, prefix, cluster_file):\n",
    "    \n",
    "    fls = glob.glob(os.path.join(result_dir,\"*.json\"))\n",
    "\n",
    "    cols = ['filename','clusterId']\n",
    "    clusters = pd.DataFrame(columns=cols)\n",
    "    moved_file_cnt = 0\n",
    "\n",
    "    for fl in fls:\n",
    "\n",
    "        f = open(fl)\n",
    "        dat = json.load(f)\n",
    "        if dat['status'] == \"succeeded\":\n",
    "            cid = dat['analyzeResult']['pageResults'][0]['clusterId']\n",
    "            flname = os.path.basename(fl)\n",
    "            tdf = pd.DataFrame([[flname, cid]], columns=cols)\n",
    "            clusters = clusters.append(tdf, ignore_index=True)\n",
    "            #print(\"clusterID for: \"+fl+\" is: \"+str(cid), not (cid is None))\n",
    "\n",
    "            if not (cid is None):\n",
    "                #move files with cluster ID to cluster folders\n",
    "                pdf_name = flname.replace(\".json\",\".pdf\")\n",
    "                fld_name = prefix+\"-\"+str(cid)\n",
    "                fld_path = os.path.join(cluster_dir,fld_name)\n",
    "                if not os.path.isdir(fld_path):\n",
    "                    os.makedirs(fld_path)\n",
    "                    print(\"creating folder:\"+fld_path)\n",
    "                if os.path.isfile(os.path.join(src_dir,pdf_name)):\n",
    "                    shutil.move(os.path.join(src_dir,pdf_name), os.path.join(fld_path,pdf_name))\n",
    "                    #print(os.path.join(src_dir,pdf_name), os.path.join(fld_path,pdf_name))\n",
    "                    moved_file_cnt = moved_file_cnt + 1\n",
    "\n",
    "    clusters.to_csv(cluster_file)\n",
    "    \n",
    "    return(moved_file_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c3d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
